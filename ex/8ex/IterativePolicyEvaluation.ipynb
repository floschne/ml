{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Implement Iterative Policy Evaluation\n",
    "Since this is a kind of **dynamic programming** algorithm, a *complete model of the environment is required*!\n",
    "\n",
    "##### Pseudocode:\n",
    "```\n",
    "1) Init V(s) and pi(s) randomly\n",
    "\n",
    "Loop until diff < theta\n",
    "    diff = 0\n",
    "    \n",
    "    for each s in S:\n",
    "        v = V(s)\n",
    "        \n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init the actions and the environment model\n",
    "A = ['l', 'r', 'u', 'd'] # set of possible actions: left, right, up, down\n",
    "A = dict(zip(A, [-1, +1, -4, +4]))\n",
    "S = np.arange(16) # all states (grid world 4x4)\n",
    "T = np.array([0, 15]) # terminal states\n",
    "\n",
    "def valid_action(s, a):\n",
    "    assert s in S\n",
    "    assert a in A\n",
    "    \n",
    "    if s + A[a] in S:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# policy : probability of taking action a given state s Pr{a | s}\n",
    "def pi(s, a):\n",
    "    assert s in S\n",
    "    assert a in A\n",
    "    \n",
    "    # since every action can be done in 3/4 of all states, the actions are all equally likely ?!\n",
    "    return 1 / len(A)\n",
    "\n",
    "def get_next_state(s, a):\n",
    "    if s in T:\n",
    "        return s\n",
    "    \n",
    "    if s + A[a] in S:\n",
    "        return s + A[a]\n",
    "    else:\n",
    "        return s\n",
    "\n",
    "def init_state_value_function():\n",
    "    V = np.zeros(16)\n",
    "    return V\n",
    "\n",
    "def R(s, s_, a): # get reward when transitioning from s to s_ with action a\n",
    "    if s_ in T or s in T:\n",
    "        return 0.\n",
    "    else:\n",
    "        return -1.\n",
    "    \n",
    "def P(s, s_, a):\n",
    "    if get_next_state(s, a) == s_:\n",
    "        return 1.\n",
    "    else:\n",
    "        return 0.\n",
    "    \n",
    "def get_next_possible_states(s):\n",
    "    S_ = []    \n",
    "    for a in A:\n",
    "        S_.append(get_next_state(s, a))\n",
    "    return S_\n",
    "    \n",
    "def print_grid(S, s = ''):\n",
    "    print(s)\n",
    "    print(S.reshape(4, 4))\n",
    "    \n",
    "    \n",
    "def calc_bellman_equation(current_state, V, gamma):\n",
    "    V_s = 0\n",
    "        \n",
    "    for action in A:\n",
    "        reward = 0.\n",
    "        possible_next_states = get_next_possible_states(current_state)\n",
    "        \n",
    "        for possible_next_state in possible_next_states:            \n",
    "            reward += P(current_state, possible_next_state, action) * (R(current_state, possible_next_state, action) + gamma * V[possible_next_state])\n",
    "    \n",
    "        V_s += pi(current_state, action) * reward\n",
    "            \n",
    "    return V_s    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterative_policy(pi, theta = .5, gamma=1., verbose=True):  \n",
    "    V = init_state_value_function()\n",
    "    V_old = V.copy()\n",
    "    \n",
    "    diff = 2\n",
    "    t = 0\n",
    "    print_grid(S, 'states')\n",
    "    while (diff > theta):\n",
    "        if verbose:\n",
    "            print_grid(V, 'state-value function after t = %i' % (t))\n",
    "        diff = 0\n",
    "        for s in S:\n",
    "            v = V[s]\n",
    "            # bellmann\n",
    "            Vs = 0\n",
    "            for a in A:\n",
    "                returnn = 0\n",
    "                for s_ in get_next_possible_states(s):\n",
    "                    returnn += P(s, s_, a) * (R(s, s_, a) + gamma * V_old[s_])\n",
    "                Vs += pi(s, a) * returnn    \n",
    "            # assert Vs == calc_bellman_equation(s, V_old, gamma)\n",
    "            V[s] = Vs\n",
    "            \n",
    "            diff = max(diff, np.abs(v - V[s]))\n",
    "        V_old = V.copy()\n",
    "        t += 1\n",
    "    \n",
    "    return V, t\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "states\n",
      "[[ 0  1  2  3]\n",
      " [ 4  5  6  7]\n",
      " [ 8  9 10 11]\n",
      " [12 13 14 15]]\n",
      "Final state-value function after t = 197 \n",
      "[[  0.         -12.27261839 -17.30287577 -17.6059027 ]\n",
      " [-13.72714947 -16.51500435 -18.03014164 -17.78771907]\n",
      " [-17.78771907 -18.03014164 -16.51500435 -13.72714947]\n",
      " [-17.6059027  -17.30287577 -12.27261839   0.        ]]\n"
     ]
    }
   ],
   "source": [
    "Vpi, t = iterative_policy(pi, 1e-5, verbose=False)\n",
    "\n",
    "print_grid(Vpi, 'Final state-value function after t = %i ' % (t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
